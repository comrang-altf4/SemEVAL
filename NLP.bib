
@inproceedings{berhe_survey_2022,
	address = {France},
	title = {Survey on {Narrative} {Structure}: from {Linguistic} {Theories} to {Automatic} {Extraction} {Approaches}},
	shorttitle = {Survey on {Narrative} {Structure}},
	url = {https://aclanthology.org/2022.tal-1.3},
	urldate = {2024-11-18},
	booktitle = {Traitement {Automatique} des {Langues}, {Volume} 63, {Numéro} 1 : {Varia} [{Varia}]},
	publisher = {ATALA (Association pour le Traitement Automatique des Langues)},
	author = {Berhe, Aman and Guinaudeau, Camille and Barras, Claude},
	editor = {Fabre, Cécile and Morin, Emmanuel and Rosset, Sophie and Sébillot, Pascale},
	year = {2022},
	pages = {63--87},
	file = {Full Text PDF:C\:\\Users\\pomec\\Zotero\\storage\\S7ESW7MH\\Berhe et al. - 2022 - Survey on Narrative Structure from Linguistic The.pdf:application/pdf},
}

@article{valls-vargas_towards_nodate,
	title = {Towards {Automatically} {Extracting} {Story} {Graphs} from {Natural} {Language} {Stories}},
	abstract = {This paper presents an approach to automatically extracting and representing narrative information from stories written in natural language. Speciﬁcally, we present our results in extracting story graphs, a formalism that captures the entities (e.g., characters, props, locations) and their interactions in a story. The long-term goal of this research is to automatically extract this narrative information in order to use it in computational narrative systems such as story generators or interactive ﬁction systems. Our approach combines narrative domain knowledge and off-the-shelf natural language processing (NLP) tools into a machine learning framework to build story graphs by automatically identifying entities, actions, and narrative roles. We report the performance of our fully automated system in a corpus of 21 stories and provide examples of the extracted story graphs and their uses in computational narrative systems.},
	language = {en},
	author = {Valls-Vargas, Josep and Zhu, Jichen and Ontanon, Santiago},
	file = {Valls-Vargas et al. - Towards Automatically Extracting Story Graphs from.pdf:C\:\\Users\\pomec\\Zotero\\storage\\YY3VS8F2\\Valls-Vargas et al. - Towards Automatically Extracting Story Graphs from.pdf:application/pdf},
}

@misc{noauthor_substituting_nodate,
	title = {Substituting {Data} {Annotation} with {Balanced} {Updates} and {Collective} {Loss} in {Multi}-label {Text} {Classification} {Citation}: {Ozmen}, {M}. {Cotnareanu}, {J}. and {Coates}, {M}. {Substituting} {Data} {Annotation} with {Balanced} {Updates} and {Collective} {Loss} in {Multi}-label {Text} {Classification}. {Proc}. {Conf}. {Lifelong} {Learning} {Agents} ({CoLLAs}), 2023. {The} implementation is available at https://github.com/muberraozmen/{BNCL}},
	shorttitle = {Substituting {Data} {Annotation} with {Balanced} {Updates} and {Collective} {Loss} in {Multi}-label {Text} {Classification} {Citation}},
	url = {https://ar5iv.labs.arxiv.org/html/2309.13543},
	abstract = {Multi-label text classification (MLTC) is the task of assigning multiple labels to a given text, and has a wide range of application domains. Most existing approaches require an enormous amount of annotated data to lea…},
	language = {en},
	urldate = {2024-11-19},
	journal = {ar5iv},
	file = {Snapshot:C\:\\Users\\pomec\\Zotero\\storage\\JQ34YVWK\\2309.html:text/html},
}

@article{xu_contrastive_2023,
	title = {Contrastive {Learning} {Models} for {Sentence} {Representations}},
	volume = {14},
	issn = {2157-6904},
	url = {https://dl.acm.org/doi/10.1145/3593590},
	doi = {10.1145/3593590},
	abstract = {Sentence representation learning is a crucial task in natural language processing, as the quality of learned representations directly influences downstream tasks, such as sentence classification and sentiment analysis. Transformer-based pretrained language models such as bidirectional encoder representations from transformers (BERT) have been extensively applied to various natural language processing tasks, and have exhibited moderately good performance. However, the anisotropy of the learned embedding space prevents BERT sentence embeddings from achieving good results in the semantic textual similarity tasks. It has been shown that contrastive learning can alleviate the anisotropy problem and significantly improve sentence representation performance. Therefore, there has been a surge in the development of models that utilize contrastive learning to fine-tune BERT-like pretrained language models to learn sentence representations. But no systematic review of contrastive learning models for sentence representations has been conducted. To fill this gap, this article summarizes and categorizes the contrastive learning based sentence representation models, common evaluation tasks for assessing the quality of learned representations, and future research directions. Furthermore, we select several representative models for exhaustive experiments to illustrate the quantitative improvement of various strategies on sentence representations.},
	number = {4},
	urldate = {2024-11-19},
	journal = {ACM Trans. Intell. Syst. Technol.},
	author = {Xu, Lingling and Xie, Haoran and Li, Zongxi and Wang, Fu Lee and Wang, Weiming and Li, Qing},
	month = jun,
	year = {2023},
	pages = {67:1--67:34},
	file = {Full Text PDF:C\:\\Users\\pomec\\Zotero\\storage\\4HDVKPKA\\Xu et al. - 2023 - Contrastive Learning Models for Sentence Represent.pdf:application/pdf},
}
